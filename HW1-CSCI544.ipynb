{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ajeya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ajeya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\ajeya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('omw-1.4')\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bs4 in c:\\users\\ajeya\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (0.0.1)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\ajeya\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (from bs4) (4.11.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\ajeya\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (from beautifulsoup4->bs4) (2.3.2.post1)\n",
      "Requirement already satisfied: pandas in c:\\users\\ajeya\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (1.1.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\ajeya\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: numpy>=1.15.4 in c:\\users\\ajeya\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (from pandas) (1.19.5)\n",
      "Requirement already satisfied: pytz>=2017.2 in c:\\users\\ajeya\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (from pandas) (2022.2.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\ajeya\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (from python-dateutil>=2.7.3->pandas) (1.16.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\ajeya\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (1.19.5)\n",
      "Requirement already satisfied: nltk in c:\\users\\ajeya\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (3.6.7)\n",
      "Requirement already satisfied: click in c:\\users\\ajeya\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\ajeya\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (from nltk) (2022.8.17)\n",
      "Requirement already satisfied: tqdm in c:\\users\\ajeya\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (from nltk) (4.64.0)\n",
      "Requirement already satisfied: joblib in c:\\users\\ajeya\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (from nltk) (1.1.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\ajeya\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (from click->nltk) (0.4.4)\n",
      "Requirement already satisfied: importlib-metadata in c:\\users\\ajeya\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (from click->nltk) (4.8.3)\n",
      "Requirement already satisfied: importlib-resources in c:\\users\\ajeya\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (from tqdm->nltk) (5.4.0)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\ajeya\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (from importlib-metadata->click->nltk) (3.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in c:\\users\\ajeya\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (from importlib-metadata->click->nltk) (4.0.1)\n",
      "Requirement already satisfied: contractions in c:\\users\\ajeya\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (0.1.72)\n",
      "Requirement already satisfied: textsearch>=0.0.21 in c:\\users\\ajeya\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (from contractions) (0.0.21)\n",
      "Requirement already satisfied: anyascii in c:\\users\\ajeya\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (from textsearch>=0.0.21->contractions) (0.3.1)\n",
      "Requirement already satisfied: pyahocorasick in c:\\users\\ajeya\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (from textsearch>=0.0.21->contractions) (1.4.4)\n",
      "Requirement already satisfied: sklearn in c:\\users\\ajeya\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (0.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\ajeya\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (from sklearn) (0.24.2)\n",
      "Requirement already satisfied: scipy>=0.19.1 in c:\\users\\ajeya\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (from scikit-learn->sklearn) (1.5.4)\n",
      "Requirement already satisfied: numpy>=1.13.3 in c:\\users\\ajeya\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (from scikit-learn->sklearn) (1.19.5)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\ajeya\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (from scikit-learn->sklearn) (3.1.0)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\ajeya\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (from scikit-learn->sklearn) (1.1.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install bs4 \n",
    "! pip install pandas\n",
    "! pip  install numpy\n",
    "! pip install nltk\n",
    "! pip install contractions\n",
    "! pip install sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data\n",
    "\n",
    "Read data as pandas dataframe providing local path for the dataset file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('C:/Users/ajeya/OneDrive - University of Southern California/Desktop/Course Files/CSCI 544 - Applied NLP/Dataset/amazon_reviews_us_Jewelry_v1_00.tsv', sep='\\t', error_bad_lines=False, warn_bad_lines=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keep Reviews and Ratings\n",
    "\n",
    "Retaining only reviews and ratings field. Also filtering reviews which has empty review body."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df[['review_body', 'star_rating']]\n",
    "data = data[data['review_body'].str.len() > 0 ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## We select 20000 reviews randomly from each rating class.\n",
    "\n",
    "Since the dataset is big, we're taking a sample of 20000 records from each class and combining the data to get 100K records.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Separating the data based on star_rating\n",
    "\n",
    "rating_1 = data[data['star_rating']==1]\n",
    "\n",
    "rating_2 = data[data['star_rating']==2]\n",
    "\n",
    "rating_3 = data[data['star_rating']==3]\n",
    "\n",
    "rating_4 = data[data['star_rating']==4]\n",
    "\n",
    "rating_5 = data[data['star_rating']==5]\n",
    "\n",
    "#Taking a sample of 20000 from each rating value\n",
    "\n",
    "rating_1 = rating_1.sample(n=20000, random_state=2)\n",
    "rating_2 = rating_2.sample(n=20000, random_state=2)\n",
    "rating_3 = rating_3.sample(n=20000, random_state=2)\n",
    "rating_4 = rating_4.sample(n=20000, random_state=2)\n",
    "rating_5 = rating_5.sample(n=20000, random_state=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning\n",
    "\n",
    "Data Cleaning includes following tasks:\n",
    "1) Remove hyperlinks\n",
    "2) Remove HTML\n",
    "3) Convert review to lowercase\n",
    "4) Fix Contractions\n",
    "5) Remove non-alphabetical characters\n",
    "6) Remove whitespaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing\n",
    "\n",
    "Data preprocessing includes following tasks:\n",
    "1) Remove stopwords\n",
    "2) Perform lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "188.25037 , 181.49296\n"
     ]
    }
   ],
   "source": [
    "import contractions\n",
    "\n",
    "#Combining data from all types of ratings\n",
    "review_data = pd.concat([rating_1, rating_2,rating_3, rating_4,rating_5]) \n",
    "review_data = review_data.sample(frac = 1)\n",
    "\n",
    "#Converting review_body to String\n",
    "review_data[\"review_body\"] = review_data[\"review_body\"].apply(str)\n",
    "total_characters_init = 0\n",
    "total_reviews = len(review_data)\n",
    "for index, row in review_data.iterrows():\n",
    "      total_characters_init = total_characters_init + len(row[\"review_body\"])\n",
    "\n",
    "#Calculating average review length before data cleaning        \n",
    "average_review_length = total_characters_init/total_reviews\n",
    "#print(\"Average Characters per review before data cleaning: \",average_review_length)\n",
    "review_len_before = average_review_length\n",
    "\n",
    "#Function for data cleaning\n",
    "def clean_review(text):\n",
    "    regex_alpha = '[^a-zA-Z]'\n",
    "    regex_html = '<.*?>'\n",
    "    regex_url = r'http[s]?://\\S+'\n",
    "    #Remove URL from reviews\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)\n",
    "    #Remove HTML from reviews\n",
    "    text = re.sub(regex_html, ' ', text)\n",
    "    #Convert to lower letters\n",
    "    text = text.lower()\n",
    "    #Fix Contractions\n",
    "    text = contractions.fix(text)\n",
    "    #Remove non alphabetical characters\n",
    "    text = re.sub(regex_alpha, ' ', text)\n",
    "    text = \" \".join(text.split())\n",
    "    return text\n",
    "    \n",
    "\n",
    "review_data[\"review_body\"] = review_data[\"review_body\"].apply(clean_review) \n",
    "\n",
    "#Calculating average review length after data cleaning\n",
    "total_characters = 0\n",
    "for index, row in review_data.iterrows():\n",
    "      total_characters = total_characters + len(row[\"review_body\"])\n",
    "\n",
    "average_review_length = total_characters/total_reviews\n",
    "#print(\"Average Characters per review after data cleaning: \",average_review_length)\n",
    "print(review_len_before,\",\",average_review_length )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## remove the stop words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## perform lemmatization  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "181.49296 , 107.18811\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "#print(\"Average Characters per review before data preprocessing: \",average_review_length)\n",
    "\n",
    "#Function for data preprocessing\n",
    "def preprocessing(text):\n",
    "    word_list = word_tokenize(text)\n",
    "    word_list = [w for w in word_list if not w.lower() in stop_words]\n",
    "    word_list = map(lemmatizer.lemmatize, word_list)\n",
    "    #stem_list = [stemmer.stem(word) for word in word_list]\n",
    "    text = (\" \").join(word_list)\n",
    "    return text\n",
    "\n",
    "review_data[\"review_body\"] = review_data[\"review_body\"].apply(preprocessing) \n",
    "\n",
    "#Calculating average length of reviews after data preprocessing\n",
    "total_characters_preprocess = 0\n",
    "for index, row in review_data.iterrows():\n",
    "      total_characters_preprocess = total_characters_preprocess + len(row[\"review_body\"])\n",
    "\n",
    "average_review_length_preprocess = total_characters_preprocess/total_reviews\n",
    "#print(\"Average Characters per review after data preprocessing: \",average_review_length_preprocess)\n",
    "print(average_review_length,\",\", average_review_length_preprocess)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Divide data set to reviews and labels\n",
    "X = review_data.review_body\n",
    "review_data['star_rating'] = review_data['star_rating'].apply(int)\n",
    "Y = review_data.star_rating\n",
    "\n",
    "#Divide data into train and test set, train set has 80% of data and test set has 20% of data.\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y,test_size = 0.2)\n",
    "\n",
    "#TF-IDF Feature Extraction\n",
    "tfidfvectorizer = TfidfVectorizer(analyzer='word',stop_words= 'english')\n",
    "tfidfvectorizer.fit(X)\n",
    "\n",
    "#print('No. of feature_words: ', len(tfidfvectorizer.get_feature_names()))\n",
    "\n",
    "#Form features set from tfidf vectorizer \n",
    "X_train_feature = tfidfvectorizer.transform(X_train)\n",
    "X_test_feature  = tfidfvectorizer.transform(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.48 , 0.52 , 0.5\n",
      "0.31 , 0.32 , 0.32\n",
      "0.31 , 0.27 , 0.29\n",
      "0.37 , 0.33 , 0.35\n",
      "0.54 , 0.6 , 0.57\n",
      "0.4 , 0.41 , 0.4\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn import metrics\n",
    "\n",
    "#Function to print results in required format\n",
    "def print_result(scores):\n",
    "    for i in range(1,6):\n",
    "        class_value = str(i)\n",
    "        precision = str(round(scores[class_value]['precision'],2))\n",
    "        recall = str(round(scores[class_value]['recall'],2))    \n",
    "        f1 = str(round(scores[class_value]['f1-score'],2))\n",
    "        print(precision,\",\",recall,\",\",f1)\n",
    "    avg_precision =  str(round(scores['weighted avg']['precision'],2)) \n",
    "    avg_recall = str(round(scores['weighted avg']['recall'],2))    \n",
    "    avg_f1 = str(round(scores['weighted avg']['f1-score'],2))\n",
    "    print(avg_precision,\",\", avg_recall,\",\", avg_f1)\n",
    "    \n",
    "#Fitting sklearn perceptron model and predicting the ratings\n",
    "perceptron = Perceptron(\n",
    "               max_iter=250,\n",
    "               tol=0.001)\n",
    "perceptron.fit(X_train_feature, Y_train)\n",
    "prediction_perceptron = perceptron.predict(X_test_feature)\n",
    "perceptron_results = metrics.classification_report(Y_test, prediction_perceptron, output_dict=True)\n",
    "\n",
    "print_result(perceptron_results)\n",
    "#print(metrics.classification_report(Y_test, prediction_perceptron))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.55 , 0.64 , 0.59\n",
      "0.38 , 0.34 , 0.36\n",
      "0.39 , 0.32 , 0.35\n",
      "0.44 , 0.4 , 0.42\n",
      "0.6 , 0.72 , 0.65\n",
      "0.47 , 0.49 , 0.48\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "#Fitting SVM model and predicting the ratings\n",
    "SVCmodel = LinearSVC(C = 0.5)\n",
    "SVCmodel.fit(X_train_feature, Y_train)\n",
    "prediction_svm = SVCmodel.predict(X_test_feature)\n",
    "\n",
    "svm_results = metrics.classification_report(Y_test, prediction_svm,output_dict=True) \n",
    "print_result(svm_results)\n",
    "#print(metrics.classification_report(Y_test, prediction_svm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.58 , 0.63 , 0.6\n",
      "0.39 , 0.38 , 0.38\n",
      "0.41 , 0.38 , 0.39\n",
      "0.46 , 0.43 , 0.44\n",
      "0.63 , 0.67 , 0.65\n",
      "0.49 , 0.5 , 0.5\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "#Fitting sklearn logistic regression model and predicting the ratings\n",
    "LogisticRegressionModel = LogisticRegression(C = 1, max_iter = 1000)\n",
    "LogisticRegressionModel.fit(X_train_feature, Y_train)\n",
    "prediction_logistic_regression = LogisticRegressionModel.predict(X_test_feature)\n",
    "\n",
    "logistic_regression_results = metrics.classification_report(Y_test, prediction_logistic_regression,output_dict=True)\n",
    "print_result(logistic_regression_results)                                                        \n",
    "#print(metrics.classification_report(Y_test, prediction_logistic_regression))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.59 , 0.58 , 0.58\n",
      "0.38 , 0.39 , 0.39\n",
      "0.39 , 0.37 , 0.38\n",
      "0.43 , 0.42 , 0.42\n",
      "0.63 , 0.66 , 0.64\n",
      "0.48 , 0.48 , 0.48\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "#Fitting sklearn Naive Bayes model and predicting the ratings\n",
    "naive_bayes_classifier = MultinomialNB()\n",
    "naive_bayes_classifier.fit(X_train_feature, Y_train)\n",
    "\n",
    "prediction = naive_bayes_classifier.predict(X_test_feature)\n",
    "\n",
    "naive_bayes_results = metrics.classification_report(Y_test, prediction,output_dict=True)\n",
    "print_result(naive_bayes_results) \n",
    "#print(metrics.classification_report(Y_test, prediction))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
